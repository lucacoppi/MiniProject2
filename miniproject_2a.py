# -*- coding: utf-8 -*-
"""MiniProject 2a

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N0wiKYR3WdIcFEeAt1lo8TpSdD0K8szf

Download the Dataset
Download the Dataset
Method 1
Download the Dataset from the following link:
Download the dataset to your local computer in the project directory of your choice.
https://www.openml.org/d/31
Method 2
Use Scikit-Learn library to directly download the dataset into your Python notebook.
Check out how?
"""

import sklearn
from sklearn import datasets
dataset = sklearn.datasets.fetch_openml(name="credit-g")
import pandas as pd

# encodes automatically the categorical variables
data = pd.DataFrame(dataset.data, columns = dataset.feature_names)
data

"""Reading the Dataset
Read the dataset into the Pandas Data Frame!
"""

#doesn't encode

df=pd.read_csv("https://www.openml.org/data/get_csv/31/dataset_31_credit-g.arff")
df

"""Does the dataset include any missing values? If so, drop them!
Hint: Pandas can do that with one line of code!
"""

df.isnull().sum().sum()
print(df[df.isna().any(axis=1)])

"""Feature Selection
Choose the features you think are relevant to our analysis! There are A LOT of features in this dataset but we have to make our models training time reasonable for you.
You MUST include at least 4 numeric features and at least 3 nominal features. You can choose more if you prefer.
"""

df1 = df.iloc[:,[2,4,5,6,10,12,17,20]]
df1

"""Preprocessing
Perform any needed pre-processing on the chosen features including:
Scaling;
Encoding; and
Dealing with Nan values.
Note:
Use only the preprocessing steps you think useful.
"""

from sklearn.preprocessing import StandardScaler
df1.loc[:,['credit_amount', 'residence_since', 'age', 'num_dependents']] = StandardScaler().fit_transform(df1[['credit_amount', 'residence_since', 'age', 'num_dependents']])
df1

from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline

class MultiColumnLabelEncoder:
    def __init__(self,columns = None):
        self.columns = columns # array of column names to encode

    def fit(self,X,y=None):
        return self # not relevant here

    def transform(self,X):
        '''
        Transforms columns of X specified in self.columns using
        LabelEncoder(). If no columns specified, transforms all
        columns in X.
        '''
        output = X.copy()
        if self.columns is not None:
            for col in self.columns:
                output[col] = LabelEncoder().fit_transform(output[col])
        else:
            for colname,col in output.iteritems():
                output[colname] = LabelEncoder().fit_transform(col)
        return output

    def fit_transform(self,X,y=None):
        return self.fit(X,y).transform(X)

df2=MultiColumnLabelEncoder(columns = ['credit_history','savings_status', 'employment', 'class']).fit_transform(df1)
print(df2)

"""Split your data as follows:
80% training set
10% validation set
10% test set
"""

y = df2.loc[:, 'class'].values
x = df2.iloc[:, 0:7].values
print(x)
print(y)

from sklearn.model_selection import train_test_split

train_ratio = 0.80
validation_ratio = 0.10
test_ratio = 0.10

# train is now 80% of the entire data set
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 1 - train_ratio)

# test is now 10% of the initial data set
# validation is now 10% of the initial data set
# test size is 50%
x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size = test_ratio/(test_ratio + validation_ratio))

"""Use the KNN-Classifier model to train your data.
Choose the best k for the K-Nearest Neighbor (KNN) algorithm by trying different values and validating performance on the validation set.
Note: Choosing the best k is an example of hyper-parameter tuning.

Classification Metrics
Print the accuracy score of your final classifier.
Print the confusion matrix.
"""

from sklearn.neighbors import KNeighborsClassifier 
K = 4
model = KNeighborsClassifier(n_neighbors=K)
model.fit(x_train, y_train)
y_pred = model.predict(x_val)

from sklearn.metrics import precision_score
from sklearn import metrics
print('precision score: '+ str(precision_score(y_val, y_pred, average='weighted')))

score = model.score(x_test,y_test) #accuracy score?
print(score)
print(metrics.confusion_matrix(y_test, y_pred))
print(metrics.classification_report(y_test, y_pred))